{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLearning Assignment 2 实验报告\n",
    "# SA22221042 汪泱泱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、实验环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU TITAN Xp  \n",
    "CUDA 10.1  \n",
    "python 3.7.13  \n",
    "torch 1.8.1  \n",
    "torchtext 0.6.0  \n",
    "spacy 3.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、实验过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先进行数据集的预处理。\n",
    "由于IMDB公开数据集“Large Movie Review Dataset“是非常常见的公开数据集，torchtext中提供了接口`torchtext.datasets.imdb.IMDB`，我们可以直接使用其进行预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/wangyy/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n"
     ]
    }
   ],
   "source": [
    "train_text = torchtext.data.Field(tokenize = 'spacy',\n",
    "                  tokenizer_language = 'en_core_web_sm',\n",
    "                  include_lengths = True)\n",
    "train_label = torchtext.data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = torchtext.datasets.imdb.IMDB.splits(train_text, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "划分验证集，划分比例为训练集：验证集=4:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=20221212\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED),split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用facebook预训练好的fasttext.en.300d的词向量编码器构建语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "train_text.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"fasttext.en.300d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "train_label.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按batch_size打包数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义各参数和超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imput_dim = len(train_text.vocab)\n",
    "vector_dim = 300\n",
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "layer_num = 3\n",
    "is_bidirectional = True\n",
    "dropout = 0.5\n",
    "pad_idx = train_text.vocab.stoi[train_text.pad_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义RNN模型，选择使用双向LSTM网络,使用torch.cat()连接两层隐藏层，并使用dropout防止网络过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx, classifier):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.classifier_head = classifier\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n",
    "        packed_output, (hidden, _) = self.rnn(packed_embedded)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        return self.classifier_head(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类头，先简单使用一层全连接层和一层sigmoid将值映射到$[0,1]$上，便于计算二分类交叉熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = torch.nn.Sequential(\n",
    "    nn.Linear(2 * hidden_dim, 1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化模型，载入预训练的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9462,  2.3363, -1.0662,  ..., -0.9242, -0.4681,  0.5014],\n",
       "        [-1.2809,  0.7598,  0.9215,  ..., -0.4192,  0.1030, -0.8865],\n",
       "        [-0.0653, -0.0930, -0.0176,  ...,  0.1664, -0.1308,  0.0354],\n",
       "        ...,\n",
       "        [-0.0119, -0.2919,  0.0028,  ..., -0.0560,  0.5747,  0.2598],\n",
       "        [ 0.4611, -0.1463, -0.6661,  ...,  0.1410,  0.3896, -0.0532],\n",
       "        [ 0.1495, -0.1802, -0.1626,  ...,  0.0777,  0.2920, -0.0942]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(imput_dim, vector_dim, hidden_dim, output_dim, layer_num, is_bidirectional, dropout, pad_idx, classifier)\n",
    "model.embedding.weight.data.copy_(train_text.vocab.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<unk>未知词标记，<pad>填充标记，这两个标记与情感无关，所以填充torch全零向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_idx = train_text.vocab.stoi[train_text.unk_token]\n",
    "model.embedding.weight.data[unk_idx] = torch.zeros(vector_dim)\n",
    "model.embedding.weight.data[pad_idx] = torch.zeros(vector_dim)\n",
    "model.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用BCELoss作为模型损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "Loss = torch.nn.BCELoss()\n",
    "model = model.to(device)\n",
    "Loss = Loss.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义计算准确率的函数，对结果取四舍五入近似和真实值比较是否相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_acc(preds, y):\n",
    "    rounded_preds = torch.round(preds)\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练代码（有反向传播更新参数）和在验证集上的上测试loss和acc的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, Loss):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, text_lengths = batch.text\n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        loss = Loss(predictions, batch.label)\n",
    "        acc = cal_acc(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, Loss):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            loss = Loss(predictions, batch.label)\n",
    "            acc = cal_acc(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练。经过实验，训练可能会在20轮作用收敛，所以我们将训练30个epoch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, Loss)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, Loss)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc:.5f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc:.5f}')\n",
    "model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三、参数选取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面修改一些网络参数训练网络后，在验证进行测试，以求找到最佳参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先改变隐藏层层数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Layer Num | Best Valid Loss |\n",
    "| ---------- | --------------- |\n",
    "| 1         | 0.293          |\n",
    "| 2        | **0.263**      |\n",
    "| 3        | 0.271          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隐藏层维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Hidden Layer Dimension  | Best Valid Loss |\n",
    "| ---------- | --------------- |\n",
    "| 128         | 0.285          |\n",
    "| 256        | **0.263**      |\n",
    "| 512        | 0.266          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词向量嵌入方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Vector Embedding Method | Best Valid Loss |\n",
    "| ---------- | --------------- |\n",
    "| fasttext.en.300d         | **0.263**          |\n",
    "| glove.6B.100d        | 0.274      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批的大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Batch Size | Best Valid Loss |\n",
    "| ---------- | --------------- |\n",
    "| 64         | 0.281          |\n",
    "| 128        | **0.263**      |\n",
    "| 256        | 0.274          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四、测试结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用验证集得到的最佳参数，在训练集上训练后，在测试集上进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "imput_dim = len(train_text.vocab)\n",
    "vector_dim = 300\n",
    "hidden_dim = 256\n",
    "output_dim = 1\n",
    "layer_num = 2\n",
    "is_bidirectional = True\n",
    "dropout = 0.5\n",
    "pad_idx = train_text.vocab.stoi[train_text.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.658 | Train Acc: 0.60052\n",
      "\t Val. Loss: 0.622 | Val. Acc: 0.67637\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.615 | Train Acc: 0.65924\n",
      "\t Val. Loss: 0.578 | Val. Acc: 0.69512\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.572 | Train Acc: 0.70233\n",
      "\t Val. Loss: 0.553 | Val. Acc: 0.70840\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.558 | Train Acc: 0.70830\n",
      "\t Val. Loss: 0.495 | Val. Acc: 0.75234\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.539 | Train Acc: 0.72930\n",
      "\t Val. Loss: 0.495 | Val. Acc: 0.77832\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.550 | Train Acc: 0.71387\n",
      "\t Val. Loss: 0.486 | Val. Acc: 0.77773\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.483 | Train Acc: 0.76747\n",
      "\t Val. Loss: 0.416 | Val. Acc: 0.81406\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.537 | Train Acc: 0.73393\n",
      "\t Val. Loss: 0.466 | Val. Acc: 0.76172\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.436 | Train Acc: 0.80300\n",
      "\t Val. Loss: 0.596 | Val. Acc: 0.78711\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.405 | Train Acc: 0.81957\n",
      "\t Val. Loss: 0.385 | Val. Acc: 0.81406\n",
      "Epoch: 11\n",
      "\tTrain Loss: 0.341 | Train Acc: 0.85211\n",
      "\t Val. Loss: 0.311 | Val. Acc: 0.86641\n",
      "Epoch: 12\n",
      "\tTrain Loss: 0.333 | Train Acc: 0.85689\n",
      "\t Val. Loss: 0.322 | Val. Acc: 0.86270\n",
      "Epoch: 13\n",
      "\tTrain Loss: 0.307 | Train Acc: 0.87062\n",
      "\t Val. Loss: 0.280 | Val. Acc: 0.88555\n",
      "Epoch: 14\n",
      "\tTrain Loss: 0.284 | Train Acc: 0.88157\n",
      "\t Val. Loss: 0.280 | Val. Acc: 0.88594\n",
      "Epoch: 15\n",
      "\tTrain Loss: 0.270 | Train Acc: 0.88873\n",
      "\t Val. Loss: 0.343 | Val. Acc: 0.87246\n",
      "Epoch: 16\n",
      "\tTrain Loss: 0.262 | Train Acc: 0.89311\n",
      "\t Val. Loss: 0.263 | Val. Acc: 0.89082\n",
      "Epoch: 17\n",
      "\tTrain Loss: 0.246 | Train Acc: 0.90058\n",
      "\t Val. Loss: 0.260 | Val. Acc: 0.89199\n",
      "Epoch: 18\n",
      "\tTrain Loss: 0.241 | Train Acc: 0.90103\n",
      "\t Val. Loss: 0.270 | Val. Acc: 0.89004\n",
      "Epoch: 19\n",
      "\tTrain Loss: 0.231 | Train Acc: 0.90824\n",
      "\t Val. Loss: 0.270 | Val. Acc: 0.89277\n",
      "Epoch: 20\n",
      "\tTrain Loss: 0.224 | Train Acc: 0.90978\n",
      "\t Val. Loss: 0.263 | Val. Acc: 0.89551\n",
      "Epoch: 21\n",
      "\tTrain Loss: 0.214 | Train Acc: 0.91546\n",
      "\t Val. Loss: 0.293 | Val. Acc: 0.89160\n",
      "Epoch: 22\n",
      "\tTrain Loss: 0.215 | Train Acc: 0.91431\n",
      "\t Val. Loss: 0.276 | Val. Acc: 0.89551\n",
      "Epoch: 23\n",
      "\tTrain Loss: 0.192 | Train Acc: 0.92392\n",
      "\t Val. Loss: 0.270 | Val. Acc: 0.89746\n",
      "Epoch: 24\n",
      "\tTrain Loss: 0.195 | Train Acc: 0.92416\n",
      "\t Val. Loss: 0.279 | Val. Acc: 0.89746\n",
      "Epoch: 25\n",
      "\tTrain Loss: 0.176 | Train Acc: 0.92989\n",
      "\t Val. Loss: 0.284 | Val. Acc: 0.89746\n",
      "Epoch: 26\n",
      "\tTrain Loss: 0.174 | Train Acc: 0.93342\n",
      "\t Val. Loss: 0.281 | Val. Acc: 0.90312\n",
      "Epoch: 27\n",
      "\tTrain Loss: 0.168 | Train Acc: 0.93412\n",
      "\t Val. Loss: 0.267 | Val. Acc: 0.89414\n",
      "Epoch: 28\n",
      "\tTrain Loss: 0.176 | Train Acc: 0.92849\n",
      "\t Val. Loss: 0.283 | Val. Acc: 0.89824\n",
      "Epoch: 29\n",
      "\tTrain Loss: 0.149 | Train Acc: 0.94263\n",
      "\t Val. Loss: 0.298 | Val. Acc: 0.89297\n",
      "Epoch: 30\n",
      "\tTrain Loss: 0.154 | Train Acc: 0.93934\n",
      "\t Val. Loss: 0.284 | Val. Acc: 0.89199\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, Loss)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, Loss)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc:.5f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择验证集上表现最好的模型参数在测试集上测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.284 | Test Acc: 0.89143\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, Loss)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACC为0.89143"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
